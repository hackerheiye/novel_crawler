# 小说爬虫程序

这是一个功能强大的小说爬虫程序，可以自动爬取网络小说的章节内容，并支持断点续传、并发下载等功能。

## 功能特点

- 支持从目录页或章节页开始爬取
- 自动识别章节顺序和内容
- 支持断点续传，中断后可继续爬取
- 并发下载，提高爬取效率
- 自动监控系统资源使用情况
- 支持自定义爬取章节数量
- 自动合并章节为完整小说文件
- 支持多种小说网站格式

## 环境要求

- Python 3.7+
- 操作系统：Windows/Linux/MacOS

## 安装步骤

1. 克隆或下载本项目到本地

2. 安装依赖包：
```bash
pip install -r requirements.txt
playwright install 或者playwright install chromium
```

## 使用方法

### 基本用法

```bash
python novel_crawler.py <小说URL> [选项]
```

### 命令行参数

- `url`：必需参数，小说的URL地址（可以是目录页或章节页）
- `-n, --num_chapters`：要爬取的章节数量，默认为0（爬取所有章节）
- `-o, --output_dir`：输出目录，默认为"novels"
- `-d, --delay`：请求延迟范围（秒），格式为"最小值 最大值"，默认为"1.0 3.0"
- `-c, --chapter`：指定URL是章节页而不是目录页
- `-r, --resume`：启用断点续传，从上次中断的地方继续爬取
- `-l, --log_level`：日志级别，可选值：DEBUG/INFO/WARNING/ERROR/CRITICAL，默认为INFO
- `-p, --concurrency`：并发数量，默认为8

### 使用示例

1. 从目录页爬取所有章节：
```bash
python novel_crawler.py https://example.com/novel/123/
```

2. 从章节页开始爬取，限制爬取10章：
```bash
python novel_crawler.py https://example.com/novel/123/456.html -c -n 10
```

3. 启用断点续传，自定义输出目录：
```bash
python novel_crawler.py https://example.com/novel/123/ -r -o my_novels
```

4. 调整爬取速度和并发数：
```bash
python novel_crawler.py https://example.com/novel/123/ -d 2.0 5.0 -p 4
```

## 输出文件

程序会在指定的输出目录下创建以下文件：

1. 每个章节的单独文件（.md格式）
2. 完整的小说文件（_完整版.md）
3. 进度文件（progress.json）

## 注意事项

1. 请合理设置爬取间隔，避免对目标网站造成过大压力
2. 建议使用代理IP进行爬取，避免IP被封禁
3. 部分网站可能有反爬虫机制，可能需要调整爬取策略
4. 请遵守网站的使用条款和版权规定

## 常见问题

1. 如果遇到爬取失败，可以：
   - 检查网络连接
   - 调整请求延迟时间
   - 降低并发数量
   - 查看日志文件了解详细错误信息

2. 如果章节顺序混乱，可以：
   - 检查日志中的章节排序信息
   - 调整章节识别规则
   - 手动修改章节顺序

## 更新日志

### V1.0.1

- 章节顺序正确：无论从哪一章开始，都能按正确顺序爬取

- 智能起始点：-c 参数真正实现从指定章节开始爬取

- 自动停止：到达最后一章后自动停止，避免无限循环

- 详细日志：提供清晰的爬取进度和状态信息

- #### 🔄 兼容性说明

  - ✅ 保持所有原有功能不变

  - ✅ 断点续传功能正常工作

  - ✅ 并发爬取功能正常

  - ✅ 广告清理功能正常

  - ✅ 章节合并功能正常

### v1.0.0

- 初始版本发布
- 支持基本的爬取功能
- 支持断点续传
- 支持并发下载
- 支持系统资源监控

## 贡献指南

欢迎提交问题和改进建议！如果您想贡献代码：

1. Fork 本仓库
2. 创建您的特性分支
3. 提交您的改动
4. 推送到您的分支
5. 创建 Pull Request

## 许可证

本项目采用 MIT 许可证。详见 [LICENSE](LICENSE) 文件。 
